[TOC]

# 图像的风格迁移2

## 简介
&#8195;&#8195;[图像的风格迁移1](http://www.aiecent.com/articleDetail?article_id=31)介绍了[A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576)和[Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155)这两篇文章。第一篇能实现图片的风格迁移，但需要花上20mins，第二篇能实现固定风格的快速迁移，耗时在秒级。这期介绍另外两篇使用不同算法的文章，都支持任意风格的迁移，且耗时都在秒级。

## AdaIN
&#8195;&#8195;在说Adain之前先简单介绍一下谷歌的另一篇文章[A learned representation for artistic style](	https://arxiv.org/abs/1610.07629)。个人理解谷歌的这篇文章的主要观点是：通过对feature map归一化结果平移、缩放从而可以生成不能风格的图像
<center>![图像风格迁移](http://www.aiecent.com/img/22.png)</center>
&#8195;&#8195;这里的x代表context，β_s和γ_s分别代表不同风格的参数。在多风格的迁移任务中，CINNet的作用就是学习不同风格的β_s和γ_s，最后再通过decode层将CINNet的输出还原成图像。  
&#8195;&#8195;回到主题论文[Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization](https://arxiv.org/abs/1703.06868)提出了AdaIN，与CIN非常相似，仅仅将β_s和γ_s换成了μy和σy。这里的x代表context feature map，μy和σy分别代表style feature map的均值和方差。同样的，最后通过decode层将AdaIN的输出还原成图像。同CIN相比，AdaIN的优势在于不需要训练大批量的风格图像就可实现任意风格的迁移。同时由于AdaIN在特征空间就完成了风格变换，因此他还能动态调整内容和风格权衡，风格插值，色彩和空间的控制。
<center>![图像风格迁移](http://www.aiecent.com/img/23.png)</center>

### Adain核心代码
```python
def adaptive_instance_normalization(content_feat, style_feat):
    assert (content_feat.size()[:2] == style_feat.size()[:2])
    size = content_feat.size()
    style_mean, style_std = calc_mean_std(style_feat)
    content_mean, content_std = calc_mean_std(content_feat)

    normalized_feat = (content_feat - content_mean.expand(
        size)) / content_std.expand(size)
    return normalized_feat * style_std.expand(size) + style_mean.expand(size)

def style_transfer(vgg, decoder, content, style, alpha=1.0,
                   interpolation_weights=None):
    assert (0.0 <= alpha <= 1.0)
    content_f = vgg(content)
    style_f = vgg(style)
    if interpolation_weights:
        _, C, H, W = content_f.size()
        feat = torch.FloatTensor(1, C, H, W).zero_().to(device)
        base_feat = adaptive_instance_normalization(content_f, style_f)
        for i, w in enumerate(interpolation_weights):
            feat = feat + w * base_feat[i:i + 1]
        content_f = content_f[0:1]
    else:
        feat = adaptive_instance_normalization(content_f, style_f)
    feat = feat * alpha + content_f * (1 - alpha)
    return decoder(feat)
```
&#8195;&#8195;来看看通过AdaIN算法出来的效果图，说实话效果并没有想象中的好。目测只做了色彩范围的迁移，输出图像与风格图片纹理不符形变严重，盲猜是decoder训练不到位造成的。
<center>![图像风格迁移](http://www.aiecent.com/img/24.png)</center>

## WCT
&#8195;&#8195;[Universal Style Transfer via Feature Transforms](https://arxiv.org/abs/1705.08086),这篇文章相当有意思，刚好介于看得懂和看不懂之间。作者大意就是通过vgg19作为encoder提取不同relu层的特征值，训练一个decoder网络可以将这些不同层的特征值还原成以前的图像，这时候如果在encoder和decoder之间加入WCT，那么就是风格迁移了。
<center>![图像风格迁移](http://www.aiecent.com/img/25.png)</center>
&#8195;&#8195;下面是我认为最精彩的地方，WCT直接变换fc以匹配fs的协方差矩阵。（fc和fs分别是内容图片和风格图片的feature map）
<center>![图像风格迁移](http://www.aiecent.com/img/26.png)</center>
&#8195;&#8195;白化操作：将fc进行如下变换即可去除图像中的色彩信息（dc是协方差矩阵fc*fc'对应的特征值，ec是特征向量）
<center>![图像风格迁移](http://www.aiecent.com/img/27.png)</center>
&#8195;&#8195;彩化操作：将上面得到的fc^进行如下变换即可将风格融合
<center>![图像风格迁移](http://www.aiecent.com/img/28.png)</center>
&#8195;&#8195;先进行白化再进行彩化，能够使变换后的特征和风格图特征的协方差矩阵匹配
<center>![图像风格迁移](http://www.aiecent.com/img/29.png)</center>

### WCT核心代码
```python
def whiten_and_color(self,cF,sF):
    cFSize = cF.size()
    c_mean = torch.mean(cF,1) # c x (h x w)
    c_mean = c_mean.unsqueeze(1).expand_as(cF)
    cF = cF - c_mean

    contentConv = torch.mm(cF,cF.t()).div(cFSize[1]-1) + torch.eye(cFSize[0]).double()
    c_u,c_e,c_v = torch.svd(contentConv,some=False)

    k_c = cFSize[0]
    for i in range(cFSize[0]):
        if c_e[i] < 0.00001:
            k_c = i
            break

    sFSize = sF.size()
    s_mean = torch.mean(sF,1)
    sF = sF - s_mean.unsqueeze(1).expand_as(sF)
    styleConv = torch.mm(sF,sF.t()).div(sFSize[1]-1)
    s_u,s_e,s_v = torch.svd(styleConv,some=False)

    k_s = sFSize[0]
    for i in range(sFSize[0]):
        if s_e[i] < 0.00001:
            k_s = i
            break

    c_d = (c_e[0:k_c]).pow(-0.5)
    step1 = torch.mm(c_v[:,0:k_c],torch.diag(c_d))
    step2 = torch.mm(step1,(c_v[:,0:k_c].t()))
    whiten_cF = torch.mm(step2,cF)

    s_d = (s_e[0:k_s]).pow(0.5)
    targetFeature = torch.mm(torch.mm(torch.mm(s_v[:,0:k_s],torch.diag(s_d)),(s_v[:,0:k_s].t())),whiten_cF)
    targetFeature = targetFeature + s_mean.unsqueeze(1).expand_as(targetFeature)
    return targetFeature
```
&#8195;&#8195;看一下WCT算法下来的效果图
<center>![图像风格迁移](http://www.aiecent.com/img/30.png)</center>