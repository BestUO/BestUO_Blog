[TOC]

# 图像的风格迁移1

## 简述
&#8195;&#8195;图像的风格迁移是一个老课题，但一直没有突破性进展，直到2015年[A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576)的问世。该篇论文算法效果显著，但缺点也很明显：只针对单个的内容图片和风格图片，并且训练时间很长。2016年的[Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155)一定程度上解决了这个问题，对任意内容图片单个风格图片进行快速风格迁移。之后的发展就是针对任意内容，任意风格的快速迁移，论文很多比如[Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization](https://arxiv.org/abs/1703.06868),[Universal Style Transfer via Feature Transforms](https://arxiv.org/abs/1705.08086)。先来简单看一下效果：
<center>![图像风格迁移](http://www.aiecent.com/img/0.png)</center>

## 固定风格固定内容
&#8195;&#8195;[A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576)的核心思想：图像的低层特征包含颜色、纹理等信息，高层特征包含位置关系内容等信息。通过VGG19分别提取style(风格图像)和content(内容图像)的features，如果不断的调整x(输入图像)使得x的lower layers features与content相似，higher layers features与style相似，那么就可以认为风格迁移成功。正巧Gram Matrix只注重风格纹理等特征而忽略空间信息，可以用来判断x与style的风格相似性。
<center>![VGG19](http://www.aiecent.com/img/640.webp)</center>
&#8195;&#8195;内容损失及风格损失定义：

```python
class ContentLoss(nn.Module):
	def __init__(self, target,):
    	super(ContentLoss, self).__init__()
    	self.target = target.detach()

def forward(self, input):
    self.loss = F.mse_loss(input, self.target)
    return input
    
def gram_matrix(input):
	a, b, c, d = input.size()  # a=batch size(=1)
	features = input.view(a * b, c * d)  # resise F_XL into \hat F_XL
	G = torch.mm(features, features.t())  # compute the gram product
	return G.div(a * b * c * d)
	
class StyleLoss(nn.Module):
	def __init__(self, target_feature):
    	super(StyleLoss, self).__init__()
    	self.target = gram_matrix(target_feature).detach()

def forward(self, input):
    G = gram_matrix(input)
    self.loss = F.mse_loss(G, self.target)
    return input
```
&#8195;&#8195;通过VGG19分别提取style(风格图像)和content(内容图像)的features
```python
content_layers_default = ['conv_4']
style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']

def get_style_model_and_losses(vgg19, normalization_mean, normalization_std,
                               style_img, content_img,
                               content_layers=content_layers_default,
                               style_layers=style_layers_default):
    cnn = copy.deepcopy(vgg19)
    normalization = Normalization(normalization_mean, normalization_std).to(device)
    content_losses = []
    style_losses = []

    model = nn.Sequential(normalization)

    i = 0  # increment every time we see a conv
    for layer in cnn.children():
        if isinstance(layer, nn.Conv2d):
            i += 1
            name = 'conv_{}'.format(i)
        elif isinstance(layer, nn.ReLU):
            name = 'relu_{}'.format(i)
            layer = nn.ReLU(inplace=False)
        elif isinstance(layer, nn.MaxPool2d):
            name = 'pool_{}'.format(i)
        elif isinstance(layer, nn.BatchNorm2d):
            name = 'bn_{}'.format(i)
        else:
            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))

        model.add_module(name, layer)

        if name in content_layers:
            target = model(content_img).detach()
            content_loss = ContentLoss(target)
            model.add_module("content_loss_{}".format(i), content_loss)
            content_losses.append(content_loss)

        if name in style_layers:
            target_feature = model(style_img).detach()
            style_loss = StyleLoss(target_feature)
            model.add_module("style_loss_{}".format(i), style_loss)
            style_losses.append(style_loss)

    for i in range(len(model) - 1, -1, -1):
        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):
            break
    model = model[:(i + 1)]
    return model, style_losses, content_losses
```
&#8195;&#8195;上段代码的line 35和line 41,ContentLoss和StyleLoss均被加入到了model，因此在执行model.forward的过程中会一起计算loss，并将结果保存在对应的self.loss中，最后的train：
```python
while run[0] <= num_steps:
  def closure():
      input_img.data.clamp_(0, 1)

      optimizer.zero_grad()
      model(input_img)
      style_score = 0
      content_score = 0

      for sl in style_losses:
          style_score += sl.loss
      for cl in content_losses:
          content_score += cl.loss
      style_score *= style_weight
      content_score *= content_weight
      loss = style_score + content_score
      loss.backward()
      run[0] += 1
      if run[0] % 50 == 0:
          print("run {}:".format(run))
          print('Style Loss : {:4f} Content Loss: {:4f}'.format(
              style_score.item(), content_score.item()))
          print()
      return style_score + content_score
  optimizer.step(closure)
input_img.data.clamp_(0, 1)
return input_img
```
## 固定风格任意内容
&#8195;&#8195;Perceptual Losses for Real-Time Style Transfer and Super-Resolution这篇文章的关键点就是提出了TransformerNet。上一篇文章通过不断改变输入图像x，使x的特征分别与content和style的特征相似从而完成风格迁移，而这篇文章则提出图像x通过TransformerNet后得到图像y，如果y的特征分别与content和style的特征相似，那么就能实现固定风格任意内容的风格迁移了，同时由于TransformerNet是预训练的网络，所以迁移速度也会得到巨大的提升。
<center>![图像风格迁移](http://www.aiecent.com/img/2.webp)</center>
&#8195;&#8195;TransformerNet的作用就是将x转换为y，原文中的结构大致如下图所示。可以看到它是由3个卷积模块，5个残差模块最后再经过3个卷积模块组成。需要注意的是最后3层卷积模块的步长是1/2这里采用的反卷积做上采样，这种方法在超分辨率任务上有很好的效果
<center>![图像风格迁移](http://www.aiecent.com/img/3.webp)</center>
&#8195;&#8195;TransformerNet去掉了40*40，在每个卷积模块中做pad，使用Pytorch的上下采样函数interpolate代替原论文中的反卷积，根据[Instance Normalization: The Missing Ingredient for Fast Stylization](https://arxiv.org/abs/1607.08022)中的说法，在风格迁移中InstanceNorm效果更好，因此使用InstanceNorm代替Batchnorm。

```python

class TransformerNet(torch.nn.Module):
    def __init__(self):
        super(TransformerNet, self).__init__()
        self.model = nn.Sequential(
            ConvBlock(3, 32, kernel_size=9, stride=1),
            ConvBlock(32, 64, kernel_size=3, stride=2),
            ConvBlock(64, 128, kernel_size=3, stride=2),
            ResidualBlock(128),
            ResidualBlock(128),
            ResidualBlock(128),
            ResidualBlock(128),
            ResidualBlock(128),
            ConvBlock(128, 64, kernel_size=3, upsample=True),
            ConvBlock(64, 32, kernel_size=3, upsample=True),
            ConvBlock(32, 3, kernel_size=9, stride=1, normalize=False, relu=False),
        )
    def forward(self, x):
        return self.model(x)

class ResidualBlock(torch.nn.Module):
    def __init__(self, channels):
        super(ResidualBlock, self).__init__()
        self.block = nn.Sequential(
            ConvBlock(channels, channels, kernel_size=3, stride=1, normalize=True, relu=True),
            ConvBlock(channels, channels, kernel_size=3, stride=1, normalize=True, relu=False),
        )

    def forward(self, x):
        return self.block(x) + x

class ConvBlock(torch.nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, upsample=False, normalize=True, relu=True):
        super(ConvBlock, self).__init__()
        self.upsample = upsample
        self.block = nn.Sequential(
            nn.ReflectionPad2d(kernel_size // 2), nn.Conv2d(in_channels, out_channels, kernel_size, stride)
        )
        self.norm = nn.InstanceNorm2d(out_channels, affine=True) if normalize else None
        self.relu = relu

    def forward(self, x):
        if self.upsample:
            x = F.interpolate(x, scale_factor=2)
        x = self.block(x)
        if self.norm is not None:
            x = self.norm(x)
        if self.relu:
            x = F.relu(x)
        return x
```

&#8195;&#8195;下期介绍另外两篇论文"Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization"和"Universal Style Transfer via Feature Transforms"，他们都能实现任意风格任意内容的迁移任务，效果么仁者见仁智者见智了。